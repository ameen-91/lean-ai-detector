{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkSpqK_CsVEb",
        "outputId": "0838b854-0d28-4111-b50f-94eb892c43c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m669.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U transformers onnx onnxruntime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained('norsu/ai-detector-test')\n",
        "tokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-v3-base')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWRpwc1yswWr",
        "outputId": "c2701ccc-a88e-47e3-8ab4-d39c0b6f3fea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:562: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "model.eval()\n",
        "start = time.time()\n",
        "input = tokenizer(\"asdsf\", return_tensors='pt')\n",
        "with torch.inference_mode():\n",
        "  print(torch.softmax(model(**input)[0],1))\n",
        "\n",
        "total = time.time() - start\n",
        "total"
      ],
      "metadata": {
        "id": "qge9erPKuH_x",
        "outputId": "e6449c9d-ce15-4331-c18d-04d67c3310be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[8.8574e-04, 9.9911e-01]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.8182132244110107"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline('text-classification', model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "7xfr8m2xMIMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "pipe(\"asdsf\")\n",
        "time.time() - start"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djV_lkDsz6Fd",
        "outputId": "9852a251-ae85-4479-a1a3-7b046f4d0cd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5589210987091064"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFig4RQhXKOU",
        "outputId": "5c5a300c-875e-426d-abab-38d7c0bc1005"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DebertaV2ForSequenceClassification(\n",
              "  (deberta): DebertaV2Model(\n",
              "    (embeddings): DebertaV2Embeddings(\n",
              "      (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
              "      (dropout): StableDropout()\n",
              "    )\n",
              "    (encoder): DebertaV2Encoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x DebertaV2Layer(\n",
              "          (attention): DebertaV2Attention(\n",
              "            (self): DisentangledSelfAttention(\n",
              "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (pos_dropout): StableDropout()\n",
              "              (dropout): StableDropout()\n",
              "            )\n",
              "            (output): DebertaV2SelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
              "              (dropout): StableDropout()\n",
              "            )\n",
              "          )\n",
              "          (intermediate): DebertaV2Intermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): DebertaV2Output(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (rel_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (pooler): ContextPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): StableDropout()\n",
              "  )\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              "  (dropout): StableDropout()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import onnx\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "\n",
        "with torch.inference_mode():\n",
        "    inputs = {\"input_ids\":torch.ones(1, 768, dtype=torch.int64),\n",
        "     \"attention_mask\":torch.ones(1, 768, dtype=torch.int64),}\n",
        "    outputs = model(**inputs)\n",
        "    symbolic_names = {0:'batch_size', 1:'max_seq_len'}\n",
        "    torch.onnx.export(model,\n",
        "                (inputs['input_ids'],\n",
        "                  inputs['attention_mask'],),\n",
        "                      r\"lean-ai-detector.onnx\",\n",
        "                      opset_version=12,\n",
        "                      input_names=['input_ids',\n",
        "                                   'input_mask',],\n",
        "                      output_names=['output'],\n",
        "                      dynamic_axes={'input_ids': symbolic_names,\n",
        "                                    'input_mask': symbolic_names,})\n",
        "\n"
      ],
      "metadata": {
        "id": "Zgu2rhPNW030"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "onnx_model_path = \"lean-ai-detector.onnx\"\n",
        "quantized_model_path = \"Q-lean-ai-detector.onnx\"\n",
        "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
        "onnx_opt_model = onnx.load(onnx_model_path)\n",
        "quantize_dynamic(onnx_model_path,\n",
        "                quantized_model_path,\n",
        "                weight_type=QuantType.QInt8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbdOhMdjYT-a",
        "outputId": "967b25ec-70e4-47e1-8e3b-16f1d5327703"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "print('ONNX full precision model size (MB):', os.path.getsize(\"lean-ai-detector.onnx\")/(1024*1024))\n",
        "print('ONNX quantized model size (MB):', os.path.getsize(\"Q-lean-ai-detector.onnx\")/(1024*1024))"
      ],
      "metadata": {
        "id": "sOCf3B0u82XX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67095615-a8ba-488c-b1f8-6c55fe40f046"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ONNX full precision model size (MB): 704.3488445281982\n",
            "ONNX quantized model size (MB): 232.57878398895264\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import onnxruntime\n",
        "\n",
        "model_path = \"Q-lean-ai-detector.onnx\"\n",
        "\n",
        "sess_options = onnxruntime.SessionOptions()\n",
        "session = onnxruntime.InferenceSession(model_path, sess_options)"
      ],
      "metadata": {
        "id": "eQ65irWBADRA"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = tokenizer(\"asdf\", return_tensors=\"np\")\n",
        "\n",
        "ort_inputs = {\n",
        "                    'input_ids':  a['input_ids'],\n",
        "                    'input_mask': a['attention_mask'],\n",
        "                }"
      ],
      "metadata": {
        "id": "5Dt2z7DuDYoH"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "logits = np.reshape(session.run(None, ort_inputs), (-1,2))"
      ],
      "metadata": {
        "id": "nB7yGCEgD0D3"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.argmax(logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giijMdz6En8h",
        "outputId": "c04027e6-697a-446e-d7ee-e2b8ada8818c"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    }
  ]
}