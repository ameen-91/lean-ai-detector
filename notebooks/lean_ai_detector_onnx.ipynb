{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DkSpqK_CsVEb",
    "outputId": "0838b854-0d28-4111-b50f-94eb892c43c0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q -U transformers onnx onnxruntime datasets sentencepiece kagglehub kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QWRpwc1yswWr",
    "outputId": "c2701ccc-a88e-47e3-8ab4-d39c0b6f3fea"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('norsu/ai-detector-test')\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-v3-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qge9erPKuH_x",
    "outputId": "e6449c9d-ce15-4331-c18d-04d67c3310be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[8.8574e-04, 9.9911e-01]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.2911760807037354"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "model.eval()\n",
    "start = time.time()\n",
    "input = tokenizer(\"asdsf\", return_tensors='pt')\n",
    "with torch.inference_mode():\n",
    "  print(torch.softmax(model(**input)[0],1))\n",
    "\n",
    "total = time.time() - start\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, f=\"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "7xfr8m2xMIMS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "703.6260967254639"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "pt_size = os.path.getsize(\"model.pt\")/(1024**2)\n",
    "pt_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "djV_lkDsz6Fd",
    "outputId": "9852a251-ae85-4479-a1a3-7b046f4d0cd5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/shanegerami/ai-vs-human-text\n",
      "License(s): other\n",
      "ai-vs-human-text.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import kagglehub\n",
    "!kaggle datasets download -d shanegerami/ai-vs-human-text\n",
    "# kagglehub.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(r\"AI_Human.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cars. Cars have been around since they became ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Transportation is a large necessity in most co...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"America's love affair with it's vehicles seem...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How often do you ride in a car? Do you drive a...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cars are a wonderful thing. They are perhaps o...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487230</th>\n",
       "      <td>Tie Face on Mars is really just a big misunder...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487231</th>\n",
       "      <td>The whole purpose of democracy is to create a ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487232</th>\n",
       "      <td>I firmly believe that governments worldwide sh...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487233</th>\n",
       "      <td>I DFN't agree with this decision because a LFT...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487234</th>\n",
       "      <td>Richard Non, Jimmy Carter, and Bob Dole and ot...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>487235 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  generated\n",
       "0       Cars. Cars have been around since they became ...        0.0\n",
       "1       Transportation is a large necessity in most co...        0.0\n",
       "2       \"America's love affair with it's vehicles seem...        0.0\n",
       "3       How often do you ride in a car? Do you drive a...        0.0\n",
       "4       Cars are a wonderful thing. They are perhaps o...        0.0\n",
       "...                                                   ...        ...\n",
       "487230  Tie Face on Mars is really just a big misunder...        0.0\n",
       "487231  The whole purpose of democracy is to create a ...        0.0\n",
       "487232  I firmly believe that governments worldwide sh...        1.0\n",
       "487233  I DFN't agree with this decision because a LFT...        0.0\n",
       "487234  Richard Non, Jimmy Carter, and Bob Dole and ot...        0.0\n",
       "\n",
       "[487235 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.astype({\"generated\":int})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "data = Dataset.from_pandas(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.rename_column(\"generated\",\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SFig4RQhXKOU",
    "outputId": "5c5a300c-875e-426d-abab-38d7c0bc1005"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6\n",
      "105.0856523513794\n"
     ]
    }
   ],
   "source": [
    "st = time.time()\n",
    "\n",
    "inputs = tokenizer(data[:100]['text'], return_tensors=\"pt\", padding=True, truncation=True, max_length = 512)\n",
    "corr = 0\n",
    "with torch.inference_mode():\n",
    "    outputs = torch.argmax(model(**inputs)[0],1)\n",
    "    for i in range(len(outputs)):\n",
    "        if outputs[i] == data[i]['label']:\n",
    "          corr +=1\n",
    "    pt_acc = corr/len(outputs)\n",
    "    print(pt_acc)\n",
    "pt_time = time.time() - st\n",
    "print(pt_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Zgu2rhPNW030"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:547: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  torch.tensor(mid - 1).type_as(relative_pos),\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:551: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  torch.ceil(torch.log(abs_pos / mid) / torch.log(torch.tensor((max_position - 1) / mid)) * (mid - 1)) + mid\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:710: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  scale = torch.sqrt(torch.tensor(query_layer.size(-1), dtype=torch.float) * scale_factor)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:710: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  scale = torch.sqrt(torch.tensor(query_layer.size(-1), dtype=torch.float) * scale_factor)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:785: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  scale = torch.sqrt(torch.tensor(pos_key_layer.size(-1), dtype=torch.float) * scale_factor)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:785: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  scale = torch.sqrt(torch.tensor(pos_key_layer.size(-1), dtype=torch.float) * scale_factor)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:797: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  scale = torch.sqrt(torch.tensor(pos_query_layer.size(-1), dtype=torch.float) * scale_factor)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:797: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  scale = torch.sqrt(torch.tensor(pos_query_layer.size(-1), dtype=torch.float) * scale_factor)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:798: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if key_layer.size(-2) != query_layer.size(-2):\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:105: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  output = input.masked_fill(rmask, torch.tensor(torch.finfo(input.dtype).min))\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "with torch.inference_mode():\n",
    "    inputs = {\"input_ids\":torch.ones(1, 512, dtype=torch.int64),\n",
    "     \"attention_mask\":torch.ones(1, 512, dtype=torch.int64),}\n",
    "    outputs = model(**inputs)\n",
    "    symbolic_names = {0:'batch_size', 1:'max_seq_len'}\n",
    "    torch.onnx.export(model,\n",
    "                (inputs['input_ids'],\n",
    "                  inputs['attention_mask'],),\n",
    "                      r\"lean-ai-detector.onnx\",\n",
    "                      opset_version=12,\n",
    "                      input_names=['input_ids',\n",
    "                                   'input_mask',],\n",
    "                      output_names=['output'],\n",
    "                      dynamic_axes={'input_ids': symbolic_names,\n",
    "                                    'input_mask': symbolic_names,})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "704.3487310409546"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_size = os.path.getsize(\"lean-ai-detector.onnx\")/(1024**2)\n",
    "onnx_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "\n",
    "model_path = \"lean-ai-detector.onnx\"\n",
    "\n",
    "sess_options = onnxruntime.SessionOptions()\n",
    "session = onnxruntime.InferenceSession(model_path, sess_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.838808,  3.19955 ]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "inputs = tokenizer(\"mad\", return_tensors=\"np\")\n",
    "\n",
    "ort_inputs = {\n",
    "                    'input_ids':  inputs['input_ids'],\n",
    "                    'input_mask': inputs['attention_mask'],\n",
    "              }\n",
    "\n",
    "logits = np.reshape(session.run(None, ort_inputs), (-1,2))\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6\n",
      "153.32597064971924\n"
     ]
    }
   ],
   "source": [
    "st = time.time()\n",
    "\n",
    "inputs = tokenizer(data[:100]['text'], return_tensors=\"np\", padding=True, truncation=True,max_length=512)\n",
    "ort_inputs = {\n",
    "                    'input_ids':  inputs['input_ids'],\n",
    "                    'input_mask': inputs['attention_mask'],\n",
    "              }\n",
    "outputs = np.argmax(np.reshape(session.run(None, ort_inputs), (-1,2)),1)\n",
    "corr = 0\n",
    "for i in range(len(outputs)):\n",
    "    if outputs[i] == data[i]['label']:\n",
    "        corr +=1\n",
    "onnx_acc = corr/len(outputs)\n",
    "print(onnx_acc)\n",
    "\n",
    "onnx_time = time.time()-st\n",
    "print(onnx_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qbdOhMdjYT-a",
    "outputId": "967b25ec-70e4-47e1-8e3b-16f1d5327703"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    }
   ],
   "source": [
    "onnx_model_path = \"lean-ai-detector.onnx\"\n",
    "quantized_model_path = \"Q-lean-ai-detector.onnx\"\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "onnx_opt_model = onnx.load(onnx_model_path)\n",
    "quantize_dynamic(onnx_model_path,\n",
    "                quantized_model_path,\n",
    "                weight_type=QuantType.QInt8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "232.57864952087402"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_size = os.path.getsize(\"Q-lean-ai-detector.onnx\")/(1024**2)\n",
    "quant_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "eQ65irWBADRA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "146.7533175945282\n"
     ]
    }
   ],
   "source": [
    "model_path = \"Q-lean-ai-detector.onnx\"\n",
    "\n",
    "sess_options = onnxruntime.SessionOptions()\n",
    "session = onnxruntime.InferenceSession(model_path, sess_options)\n",
    "\n",
    "st = time.time()\n",
    "\n",
    "inputs = tokenizer(data[:100]['text'], return_tensors=\"np\", padding=True, truncation=True, max_length=512)\n",
    "ort_inputs = {\n",
    "                    'input_ids':  inputs['input_ids'],\n",
    "                    'input_mask': inputs['attention_mask'],\n",
    "              }\n",
    "outputs = np.argmax(np.reshape(session.run(None, ort_inputs), (-1,2)),1)\n",
    "corr = 0\n",
    "for i in range(len(outputs)):\n",
    "    if outputs[i] == data[i]['label']:\n",
    "        corr +=1\n",
    "quant_acc = corr/len(outputs)\n",
    "print(quant_acc)\n",
    "\n",
    "quant_time = time.time() - st\n",
    "print(quant_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "5Dt2z7DuDYoH"
   },
   "outputs": [],
   "source": [
    "a = tokenizer(\"asdf\", return_tensors=\"np\")\n",
    "\n",
    "ort_inputs = {\n",
    "                    'input_ids':  a['input_ids'],\n",
    "                    'input_mask': a['attention_mask'],\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "nB7yGCEgD0D3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "logits = np.reshape(session.run(None, ort_inputs), (-1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "giijMdz6En8h",
    "outputId": "c04027e6-697a-446e-d7ee-e2b8ada8818c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(logits)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
